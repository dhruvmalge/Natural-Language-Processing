{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8f8896",
   "metadata": {},
   "source": [
    "# NLP Steps\n",
    "\n",
    "### Tokenizing\n",
    "### Filtering Stop Words\n",
    "### Stemming\n",
    "### Tagging Parts of Speech\n",
    "### Named Entity Recognition also known as Chunking\n",
    "### Using Named Entity Recognition (NER)\n",
    "### Getting Text to Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d0e9d",
   "metadata": {},
   "source": [
    "#### Using NLP we are going to do a sentiment analysis of McDonald's review, in which we are taking some positive, negative and neutral reviews and process them to see it's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c9ba6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import warnings library to ignore all the warnings that encounter during program execution\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import necessary nlp libraries and its modules\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as wordtoken\n",
    "from nltk.tokenize import sent_tokenize as sentensetoken\n",
    "from nltk.corpus import stopwords as stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.chunk import ne_chunk as chunk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "#import tfidf \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#import bag of words \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#download all words lists\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "497dadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a nlp algorithm to work for your analysis\n",
    "\n",
    "#filter your words with tokenizations\n",
    "def word_processing(text):\n",
    "    text = text.lower()\n",
    "    tokens = wordtoken(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "#to display the word counts \n",
    "def freq_distribution(tokens):\n",
    "    freq = FreqDist(tokens)\n",
    "    freq = freq.most_common()\n",
    "    return freq\n",
    "\n",
    "#to tag parts of speech\n",
    "def pos(tokens):\n",
    "    speech = pos_tag(tokens)\n",
    "    return speech\n",
    "\n",
    "#define the named entity such as Noun, Verb,etc\n",
    "def name_entity(text):\n",
    "    token = pos_tag(wordtoken(text))\n",
    "    named_entity = chunk(token)\n",
    "    return named_entity\n",
    "\n",
    "#to analyze and get sentiment scores of negative,positive and neutral along with compound\n",
    "def sentiment_analyze(text):\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "    sentiment = sentiment.polarity_scores(text)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be6a1c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text filters :  ['excellent', 'location', 'good', 'atmosphere', ',', 'excellent', 'service']\n",
      "Frequency Distribution or Stemming :  [('excellent', 2), ('location', 1), ('good', 1), ('atmosphere', 1), (',', 1), ('service', 1)]\n",
      "Parts of Speech :  [('excellent', 'JJ'), ('location', 'NN'), ('good', 'NN'), ('atmosphere', 'NN'), (',', ','), ('excellent', 'JJ'), ('service', 'NN')]\n",
      "Named Entity relationship :  (S\n",
      "  Excellent/JJ\n",
      "  location/NN\n",
      "  and/CC\n",
      "  very/RB\n",
      "  good/JJ\n",
      "  atmosphere/NN\n",
      "  ,/,\n",
      "  excellent/JJ\n",
      "  service/NN)\n",
      "Semntiment Analysis Scores :  {'neg': 0.0, 'neu': 0.315, 'pos': 0.685, 'compound': 0.8969}\n"
     ]
    }
   ],
   "source": [
    "#put the reviews texts of your choice \n",
    "neutral_text = \"Hey, There I am in McDonalds Fast Food Chain\"\n",
    "neutral_text_2 = \"Decent McDonald's location.  The team here has always been friendly.  Drive thru can get pretty backed up during normal meal periods, so often it is better to go inside.  Make sure you go close to regular meal periods for fresher food vs the ones under the heat lamp for a while.Store is always clean.\"\n",
    "too_negative = \"One star because everytime I go early in the morning this lady is always so rude. Granted I know its early but if u can't handle being up this early then maybe you should consider getting a different job. It's not okay to be rude.\"\n",
    "too_negative_2 = \"They treated me very badly.\"\n",
    "too_positive = \"The staff are very friendly and they do their job perfectly\"\n",
    "too_positive_2 = \"Excellent location and very good atmosphere, excellent service\"\n",
    "\n",
    "# Taking the examples from above reviews texts\n",
    "filters = word_processing(too_positive_2)\n",
    "freq = freq_distribution(filters)\n",
    "parts_of_speech = pos(filters)\n",
    "entity = name_entity(too_positive_2)\n",
    "senti_analysis = sentiment_analyze(too_positive_2)\n",
    "\n",
    "#print all the features\n",
    "print(\"Text filters : \",filters)\n",
    "print(\"Frequency Distribution or Stemming : \", freq)\n",
    "print(\"Parts of Speech : \", parts_of_speech)\n",
    "print(\"Named Entity relationship : \", entity)\n",
    "print(\"Semntiment Analysis Scores : \", senti_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "270b029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary list :  {'hey': 36, 'there': 75, 'am': 1, 'in': 38, 'mcdonalds': 51, 'fast': 23, 'food': 24, 'chain': 12, 'decent': 16, 'mcdonald': 50, 'location': 47, 'the': 72, 'team': 71, 'here': 35, 'has': 33, 'always': 0, 'been': 7, 'friendly': 27, 'drive': 19, 'thru': 78, 'can': 11, 'get': 28, 'pretty': 62, 'backed': 4, 'up': 81, 'during': 20, 'normal': 54, 'meal': 52, 'periods': 61, 'so': 66, 'often': 56, 'it': 41, 'is': 40, 'better': 9, 'to': 79, 'go': 30, 'inside': 39, 'make': 48, 'sure': 70, 'you': 85, 'close': 14, 'regular': 63, 'for': 25, 'fresher': 26, 'vs': 83, 'ones': 59, 'under': 80, 'heat': 34, 'lamp': 46, 'while': 84, 'store': 69, 'clean': 13, 'staff': 67, 'are': 3, 'very': 82, 'and': 2, 'they': 76, 'do': 18, 'their': 73, 'job': 43, 'perfectly': 60, 'one': 58, 'star': 68, 'because': 6, 'everytime': 22, 'early': 21, 'morning': 53, 'this': 77, 'lady': 45, 'rude': 64, 'granted': 31, 'know': 44, 'its': 42, 'but': 10, 'if': 37, 'handle': 32, 'being': 8, 'then': 74, 'maybe': 49, 'should': 65, 'consider': 15, 'getting': 29, 'different': 17, 'not': 55, 'okay': 57, 'be': 5}\n",
      "Features names :  ['always' 'am' 'and' 'are' 'backed' 'be' 'because' 'been' 'being' 'better'\n",
      " 'but' 'can' 'chain' 'clean' 'close' 'consider' 'decent' 'different' 'do'\n",
      " 'drive' 'during' 'early' 'everytime' 'fast' 'food' 'for' 'fresher'\n",
      " 'friendly' 'get' 'getting' 'go' 'granted' 'handle' 'has' 'heat' 'here'\n",
      " 'hey' 'if' 'in' 'inside' 'is' 'it' 'its' 'job' 'know' 'lady' 'lamp'\n",
      " 'location' 'make' 'maybe' 'mcdonald' 'mcdonalds' 'meal' 'morning'\n",
      " 'normal' 'not' 'often' 'okay' 'one' 'ones' 'perfectly' 'periods' 'pretty'\n",
      " 'regular' 'rude' 'should' 'so' 'staff' 'star' 'store' 'sure' 'team' 'the'\n",
      " 'their' 'then' 'there' 'they' 'this' 'thru' 'to' 'under' 'up' 'very' 'vs'\n",
      " 'while' 'you']\n",
      "[[0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 2 1 1 1 0 2 0 0 1 1 1\n",
      "  0 0 0 1 2 1 0 0 0 0 1 1 1 0 1 0 2 0 1 0 1 0 0 1 0 2 1 1 0 0 1 0 0 1 1 1\n",
      "  3 0 0 0 0 0 1 2 1 1 0 1 1 1]\n",
      " [0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      "  1 1 0 0 1 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 3 1 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
      "  0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 2 1 1 0 1 0 0 0\n",
      "  1 0 1 0 0 2 0 1 0 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#make a continous bag of words\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "sentense = [neutral_text, neutral_text_2, too_positive, too_negative]\n",
    "\n",
    "BOW = cv.fit_transform(sentense).toarray()\n",
    "vocab = cv.vocabulary_\n",
    "features_names = cv.get_feature_names_out()\n",
    "\n",
    "print(\"Vocabulary list : \",vocab)\n",
    "print(\"Features names : \",features_names)\n",
    "print(BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56c3b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vocabulary :  {'hey': 36, 'there': 75, 'am': 1, 'in': 38, 'mcdonalds': 51, 'fast': 23, 'food': 24, 'chain': 12, 'decent': 16, 'mcdonald': 50, 'location': 47, 'the': 72, 'team': 71, 'here': 35, 'has': 33, 'always': 0, 'been': 7, 'friendly': 27, 'drive': 19, 'thru': 78, 'can': 11, 'get': 28, 'pretty': 62, 'backed': 4, 'up': 81, 'during': 20, 'normal': 54, 'meal': 52, 'periods': 61, 'so': 66, 'often': 56, 'it': 41, 'is': 40, 'better': 9, 'to': 79, 'go': 30, 'inside': 39, 'make': 48, 'sure': 70, 'you': 85, 'close': 14, 'regular': 63, 'for': 25, 'fresher': 26, 'vs': 83, 'ones': 59, 'under': 80, 'heat': 34, 'lamp': 46, 'while': 84, 'store': 69, 'clean': 13, 'staff': 67, 'are': 3, 'very': 82, 'and': 2, 'they': 76, 'do': 18, 'their': 73, 'job': 43, 'perfectly': 60, 'one': 58, 'star': 68, 'because': 6, 'everytime': 22, 'early': 21, 'morning': 53, 'this': 77, 'lady': 45, 'rude': 64, 'granted': 31, 'know': 44, 'its': 42, 'but': 10, 'if': 37, 'handle': 32, 'being': 8, 'then': 74, 'maybe': 49, 'should': 65, 'consider': 15, 'getting': 29, 'different': 17, 'not': 55, 'okay': 57, 'be': 5}\n",
      "TF-IDF features :  ['always' 'am' 'and' 'are' 'backed' 'be' 'because' 'been' 'being' 'better'\n",
      " 'but' 'can' 'chain' 'clean' 'close' 'consider' 'decent' 'different' 'do'\n",
      " 'drive' 'during' 'early' 'everytime' 'fast' 'food' 'for' 'fresher'\n",
      " 'friendly' 'get' 'getting' 'go' 'granted' 'handle' 'has' 'heat' 'here'\n",
      " 'hey' 'if' 'in' 'inside' 'is' 'it' 'its' 'job' 'know' 'lady' 'lamp'\n",
      " 'location' 'make' 'maybe' 'mcdonald' 'mcdonalds' 'meal' 'morning'\n",
      " 'normal' 'not' 'often' 'okay' 'one' 'ones' 'perfectly' 'periods' 'pretty'\n",
      " 'regular' 'rude' 'should' 'so' 'staff' 'star' 'store' 'sure' 'team' 'the'\n",
      " 'their' 'then' 'there' 'they' 'this' 'thru' 'to' 'under' 'up' 'very' 'vs'\n",
      " 'while' 'you']\n",
      "TF-IDF parameters :  {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 1), 'norm': None, 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}\n"
     ]
    }
   ],
   "source": [
    "#make a TF-IDF(Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "vectorize = TfidfVectorizer(norm=None)\n",
    "\n",
    "tfidf = vectorize.fit_transform(sentense).toarray()\n",
    "tfidf_vocab = vectorize.vocabulary_\n",
    "tfidf_feature_names = vectorize.get_feature_names_out()\n",
    "tfidf_params = vectorize.get_params()\n",
    "\n",
    "print(\"TF-IDF vocabulary : \",tfidf_vocab)\n",
    "print(\"TF-IDF features : \",tfidf_feature_names)\n",
    "print(\"TF-IDF parameters : \",tfidf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1a18e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
